{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znsh8PV8avg8"
      },
      "source": [
        "# Large Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBiSMmcdbweO"
      },
      "source": [
        "Dass es sich bei Large Language Models (LLMs) nicht um Magie oder eine starke künstliche Intelligenz (oder gar einer Artificial Generel Intelligence bzw. AGI) sondern um reine Stastistik handelt, sollte mittlerweile allen bekannt sein.\n",
        "\n",
        "Ein LLM macht doch nichts anderes, als an den vorhandenen Text ein weiteres Wort anzuhängen. Wort für Wort. Basierend auf Statistiken, die angeben, welches Wort die höchste Wahrscheinlichkeit hat, auf das oder die vorangegangenen Worte zu folgen. **Oder?!**\n",
        "\n",
        "Ich möchte versuchen mich der Funktionsweise von Natural Language Processing (NLP) und LLMs anzunähern. Dabei werde ich über Begriffe, wie **Embeddings**, **Tokens**, **Attention** oder **Transformer** stolpern und versuchen zu erklären was hier passiert. Ich werde auch kurz auf **Rekurrente Neuronale Netze** eingehen, spielten sie doch zum Beispiel im Google Translator in Form des **Google Neural Machine Translation (GNMT)** Systems eine Rolle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoiQYG5t_Yhe"
      },
      "source": [
        "## Ein Wort nach dem anderem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2CsuNRv_b3g"
      },
      "source": [
        "Language Models versuchen an den gegebenen Text sinnvoll Wörter anzuhängen. Unter sinnvoll versteht man, was man von einer Person zu schreiben erwarten würde, wenn diese Person Milliarden von Text, Büchern, Internetseiten usw. gelesen und sich dann gemerkt hätte, welches Wort am häufigsten auf den vorangegangenen Text oder das vorangegangene Wort folgte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-cpp-python in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (0.1.78)\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.75.tar.gz (48.7 MB)\n",
            "     ---------------------------------------- 0.0/48.7 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.3/48.7 MB 5.2 MB/s eta 0:00:10\n",
            "     ---------------------------------------- 0.6/48.7 MB 6.2 MB/s eta 0:00:08\n",
            "      --------------------------------------- 1.0/48.7 MB 7.0 MB/s eta 0:00:07\n",
            "     - -------------------------------------- 1.3/48.7 MB 6.8 MB/s eta 0:00:08\n",
            "     - -------------------------------------- 1.7/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     - -------------------------------------- 2.0/48.7 MB 7.0 MB/s eta 0:00:07\n",
            "     - -------------------------------------- 2.3/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     -- ------------------------------------- 2.7/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     -- ------------------------------------- 3.0/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     -- ------------------------------------- 3.4/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     --- ------------------------------------ 3.7/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     --- ------------------------------------ 4.0/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     --- ------------------------------------ 4.3/48.7 MB 7.0 MB/s eta 0:00:07\n",
            "     --- ------------------------------------ 4.6/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 5.0/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 5.3/48.7 MB 7.0 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 5.7/48.7 MB 7.1 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 5.9/48.7 MB 7.0 MB/s eta 0:00:07\n",
            "     ----- ---------------------------------- 6.2/48.7 MB 7.0 MB/s eta 0:00:07\n",
            "     ----- ---------------------------------- 6.6/48.7 MB 7.0 MB/s eta 0:00:07\n",
            "     ----- ---------------------------------- 6.9/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ----- ---------------------------------- 7.2/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ------ --------------------------------- 7.6/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ------ --------------------------------- 7.9/48.7 MB 7.1 MB/s eta 0:00:06\n",
            "     ------ --------------------------------- 8.2/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ------- -------------------------------- 8.6/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ------- -------------------------------- 8.9/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ------- -------------------------------- 9.2/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ------- -------------------------------- 9.5/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 9.9/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 10.2/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 10.5/48.7 MB 7.1 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 10.9/48.7 MB 7.1 MB/s eta 0:00:06\n",
            "     --------- ------------------------------ 11.2/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     --------- ------------------------------ 11.6/48.7 MB 7.1 MB/s eta 0:00:06\n",
            "     --------- ------------------------------ 11.9/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     --------- ------------------------------ 12.2/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ---------- ----------------------------- 12.5/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ---------- ----------------------------- 12.9/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ---------- ----------------------------- 13.2/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ----------- ---------------------------- 13.5/48.7 MB 7.0 MB/s eta 0:00:06\n",
            "     ----------- ---------------------------- 13.9/48.7 MB 7.0 MB/s eta 0:00:05\n",
            "     ----------- ---------------------------- 14.2/48.7 MB 7.0 MB/s eta 0:00:05\n",
            "     ----------- ---------------------------- 14.5/48.7 MB 7.0 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 14.7/48.7 MB 7.0 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 15.1/48.7 MB 6.9 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 15.3/48.7 MB 6.9 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 15.6/48.7 MB 6.9 MB/s eta 0:00:05\n",
            "     ------------- -------------------------- 15.8/48.7 MB 6.7 MB/s eta 0:00:05\n",
            "     ------------- -------------------------- 16.0/48.7 MB 6.7 MB/s eta 0:00:05\n",
            "     ------------- -------------------------- 16.2/48.7 MB 6.7 MB/s eta 0:00:05\n",
            "     ------------- -------------------------- 16.4/48.7 MB 6.5 MB/s eta 0:00:05\n",
            "     ------------- -------------------------- 16.8/48.7 MB 6.6 MB/s eta 0:00:05\n",
            "     ------------- -------------------------- 17.0/48.7 MB 6.5 MB/s eta 0:00:05\n",
            "     -------------- ------------------------- 17.3/48.7 MB 6.4 MB/s eta 0:00:05\n",
            "     -------------- ------------------------- 17.4/48.7 MB 6.4 MB/s eta 0:00:05\n",
            "     -------------- ------------------------- 17.7/48.7 MB 6.3 MB/s eta 0:00:05\n",
            "     -------------- ------------------------- 18.0/48.7 MB 6.3 MB/s eta 0:00:05\n",
            "     --------------- ------------------------ 18.3/48.7 MB 6.4 MB/s eta 0:00:05\n",
            "     --------------- ------------------------ 18.6/48.7 MB 6.4 MB/s eta 0:00:05\n",
            "     --------------- ------------------------ 18.9/48.7 MB 6.4 MB/s eta 0:00:05\n",
            "     --------------- ------------------------ 19.2/48.7 MB 6.4 MB/s eta 0:00:05\n",
            "     ---------------- ----------------------- 19.5/48.7 MB 6.4 MB/s eta 0:00:05\n",
            "     ---------------- ----------------------- 19.9/48.7 MB 6.3 MB/s eta 0:00:05\n",
            "     ---------------- ----------------------- 20.1/48.7 MB 6.3 MB/s eta 0:00:05\n",
            "     ---------------- ----------------------- 20.5/48.7 MB 6.3 MB/s eta 0:00:05\n",
            "     ---------------- ----------------------- 20.7/48.7 MB 6.2 MB/s eta 0:00:05\n",
            "     ----------------- ---------------------- 21.0/48.7 MB 6.2 MB/s eta 0:00:05\n",
            "     ----------------- ---------------------- 21.3/48.7 MB 6.2 MB/s eta 0:00:05\n",
            "     ----------------- ---------------------- 21.5/48.7 MB 6.1 MB/s eta 0:00:05\n",
            "     ----------------- ---------------------- 21.9/48.7 MB 6.1 MB/s eta 0:00:05\n",
            "     ------------------ --------------------- 22.1/48.7 MB 6.1 MB/s eta 0:00:05\n",
            "     ------------------ --------------------- 22.5/48.7 MB 6.1 MB/s eta 0:00:05\n",
            "     ------------------ --------------------- 22.8/48.7 MB 6.1 MB/s eta 0:00:05\n",
            "     ------------------ --------------------- 23.0/48.7 MB 6.1 MB/s eta 0:00:05\n",
            "     ------------------- -------------------- 23.3/48.7 MB 6.1 MB/s eta 0:00:05\n",
            "     ------------------- -------------------- 23.7/48.7 MB 6.0 MB/s eta 0:00:05\n",
            "     ------------------- -------------------- 23.9/48.7 MB 6.0 MB/s eta 0:00:05\n",
            "     ------------------- -------------------- 24.2/48.7 MB 5.9 MB/s eta 0:00:05\n",
            "     -------------------- ------------------- 24.5/48.7 MB 6.0 MB/s eta 0:00:05\n",
            "     -------------------- ------------------- 24.8/48.7 MB 6.0 MB/s eta 0:00:05\n",
            "     -------------------- ------------------- 25.1/48.7 MB 6.0 MB/s eta 0:00:04\n",
            "     -------------------- ------------------- 25.4/48.7 MB 6.0 MB/s eta 0:00:04\n",
            "     --------------------- ------------------ 25.7/48.7 MB 6.0 MB/s eta 0:00:04\n",
            "     --------------------- ------------------ 25.9/48.7 MB 6.0 MB/s eta 0:00:04\n",
            "     --------------------- ------------------ 26.2/48.7 MB 6.1 MB/s eta 0:00:04\n",
            "     --------------------- ------------------ 26.5/48.7 MB 6.1 MB/s eta 0:00:04\n",
            "     --------------------- ------------------ 26.8/48.7 MB 6.1 MB/s eta 0:00:04\n",
            "     ---------------------- ----------------- 27.1/48.7 MB 6.2 MB/s eta 0:00:04\n",
            "     ---------------------- ----------------- 27.3/48.7 MB 6.1 MB/s eta 0:00:04\n",
            "     ---------------------- ----------------- 27.6/48.7 MB 6.2 MB/s eta 0:00:04\n",
            "     ---------------------- ----------------- 27.9/48.7 MB 6.2 MB/s eta 0:00:04\n",
            "     ----------------------- ---------------- 28.2/48.7 MB 6.2 MB/s eta 0:00:04\n",
            "     ----------------------- ---------------- 28.5/48.7 MB 6.2 MB/s eta 0:00:04\n",
            "     ----------------------- ---------------- 28.8/48.7 MB 6.2 MB/s eta 0:00:04\n",
            "     ----------------------- ---------------- 29.0/48.7 MB 6.2 MB/s eta 0:00:04\n",
            "     ------------------------ --------------- 29.3/48.7 MB 6.2 MB/s eta 0:00:04\n",
            "     ------------------------ --------------- 29.6/48.7 MB 6.2 MB/s eta 0:00:04\n",
            "     ------------------------ --------------- 29.8/48.7 MB 6.1 MB/s eta 0:00:04\n",
            "     ------------------------ --------------- 30.1/48.7 MB 6.1 MB/s eta 0:00:04\n",
            "     ------------------------- -------------- 30.4/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     ------------------------- -------------- 30.7/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     ------------------------- -------------- 31.0/48.7 MB 6.2 MB/s eta 0:00:03\n",
            "     ------------------------- -------------- 31.3/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     ------------------------- -------------- 31.6/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     -------------------------- ------------- 31.9/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     -------------------------- ------------- 32.2/48.7 MB 6.2 MB/s eta 0:00:03\n",
            "     -------------------------- ------------- 32.5/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     -------------------------- ------------- 32.8/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 33.1/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 33.3/48.7 MB 6.2 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 33.6/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 33.9/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     ---------------------------- ----------- 34.2/48.7 MB 6.2 MB/s eta 0:00:03\n",
            "     ---------------------------- ----------- 34.5/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     ---------------------------- ----------- 34.8/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     ---------------------------- ----------- 35.1/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     ----------------------------- ---------- 35.4/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     ----------------------------- ---------- 35.7/48.7 MB 6.1 MB/s eta 0:00:03\n",
            "     ----------------------------- ---------- 36.0/48.7 MB 6.2 MB/s eta 0:00:03\n",
            "     ----------------------------- ---------- 36.2/48.7 MB 6.2 MB/s eta 0:00:03\n",
            "     ------------------------------ --------- 36.6/48.7 MB 6.1 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 36.9/48.7 MB 6.2 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 37.2/48.7 MB 6.2 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 37.5/48.7 MB 6.2 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 37.8/48.7 MB 6.2 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 38.2/48.7 MB 6.3 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 38.5/48.7 MB 6.3 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 38.8/48.7 MB 6.3 MB/s eta 0:00:02\n",
            "     -------------------------------- ------- 39.2/48.7 MB 6.4 MB/s eta 0:00:02\n",
            "     -------------------------------- ------- 39.5/48.7 MB 6.4 MB/s eta 0:00:02\n",
            "     -------------------------------- ------- 39.8/48.7 MB 6.5 MB/s eta 0:00:02\n",
            "     -------------------------------- ------- 40.1/48.7 MB 6.5 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 40.4/48.7 MB 6.5 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 40.8/48.7 MB 6.5 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 41.1/48.7 MB 6.5 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 41.5/48.7 MB 6.6 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 41.7/48.7 MB 6.6 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 42.0/48.7 MB 6.6 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 42.4/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 42.6/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 42.9/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 43.2/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 43.5/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 43.8/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 44.1/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 44.4/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 44.7/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 45.1/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 45.4/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 45.7/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 46.0/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 46.3/48.7 MB 6.7 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 46.6/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 46.9/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 47.2/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 47.5/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  47.7/48.7 MB 6.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  48.0/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------  48.3/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------  48.6/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------  48.7/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------  48.7/48.7 MB 6.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 48.7/48.7 MB 6.1 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "     ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "     ---------- ----------------------------- 0.3/1.0 MB 5.4 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 0.5/1.0 MB 5.7 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 0.9/1.0 MB 6.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.0/1.0 MB 6.4 MB/s eta 0:00:00\n",
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.6.0-py3-none-win_amd64.whl (6.3 MB)\n",
            "     ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.3/6.3 MB 8.3 MB/s eta 0:00:01\n",
            "     --- ------------------------------------ 0.6/6.3 MB 6.5 MB/s eta 0:00:01\n",
            "     ----- ---------------------------------- 0.9/6.3 MB 7.0 MB/s eta 0:00:01\n",
            "     ------- -------------------------------- 1.2/6.3 MB 7.1 MB/s eta 0:00:01\n",
            "     --------- ------------------------------ 1.5/6.3 MB 7.0 MB/s eta 0:00:01\n",
            "     ------------ --------------------------- 1.9/6.3 MB 7.1 MB/s eta 0:00:01\n",
            "     -------------- ------------------------- 2.2/6.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ---------------- ----------------------- 2.5/6.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 2.9/6.3 MB 7.0 MB/s eta 0:00:01\n",
            "     -------------------- ------------------- 3.2/6.3 MB 7.0 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 3.5/6.3 MB 6.9 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 3.9/6.3 MB 7.0 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 4.2/6.3 MB 7.0 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 4.5/6.3 MB 7.0 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 4.7/6.3 MB 6.9 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 5.1/6.3 MB 6.9 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 5.4/6.3 MB 6.9 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 5.7/6.3 MB 6.9 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 6.0/6.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  6.3/6.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 6.3/6.3 MB 6.6 MB/s eta 0:00:00\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.37-py3-none-any.whl (6.8 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "     ---------------------------------------- 0.0/171.5 kB ? eta -:--:--\n",
            "     -------------------------------------- 171.5/171.5 kB 5.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from llama-cpp-python) (1.23.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
            "Collecting langchain-community<0.1,>=0.0.38\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "     ------ --------------------------------- 0.3/2.0 MB 9.6 MB/s eta 0:00:01\n",
            "     --------- ------------------------------ 0.5/2.0 MB 6.3 MB/s eta 0:00:01\n",
            "     ------------- -------------------------- 0.7/2.0 MB 5.5 MB/s eta 0:00:01\n",
            "     -------------------- ------------------- 1.0/2.0 MB 5.9 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 1.3/2.0 MB 6.1 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 1.7/2.0 MB 6.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------  2.0/2.0 MB 6.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.0/2.0 MB 6.2 MB/s eta 0:00:00\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting pydantic<3,>=1\n",
            "  Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
            "     ---------------------------------------- 0.0/409.3 kB ? eta -:--:--\n",
            "     -------------------- ----------------- 225.3/409.3 kB 6.9 MB/s eta 0:00:01\n",
            "     -------------------------------------- 409.3/409.3 kB 6.4 MB/s eta 0:00:00\n",
            "Collecting langsmith<0.2.0,>=0.1.17\n",
            "  Downloading langsmith-0.1.59-py3-none-any.whl (121 kB)\n",
            "     ---------------------------------------- 0.0/121.2 kB ? eta -:--:--\n",
            "     -------------------------------------- 121.2/121.2 kB 3.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from langchain) (3.8.3)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from langchain) (2.28.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from langchain) (6.0)\n",
            "Collecting langchain-core<0.2.0,>=0.1.52\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "     ---------------------------------------- 0.0/302.9 kB ? eta -:--:--\n",
            "     ---------------------------------- --- 276.5/302.9 kB 8.6 MB/s eta 0:00:01\n",
            "     -------------------------------------- 302.9/302.9 kB 6.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gpt4all) (4.65.0)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4\n",
            "  Downloading llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.35\n",
            "  Downloading llama_index_core-0.10.37-py3-none-any.whl (15.4 MB)\n",
            "     ---------------------------------------- 0.0/15.4 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.3/15.4 MB 6.3 MB/s eta 0:00:03\n",
            "     - -------------------------------------- 0.6/15.4 MB 7.7 MB/s eta 0:00:02\n",
            "     -- ------------------------------------- 0.9/15.4 MB 6.9 MB/s eta 0:00:03\n",
            "     --- ------------------------------------ 1.2/15.4 MB 6.3 MB/s eta 0:00:03\n",
            "     --- ------------------------------------ 1.4/15.4 MB 6.0 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 1.7/15.4 MB 6.5 MB/s eta 0:00:03\n",
            "     ----- ---------------------------------- 2.0/15.4 MB 6.4 MB/s eta 0:00:03\n",
            "     ------ --------------------------------- 2.3/15.4 MB 6.5 MB/s eta 0:00:03\n",
            "     ------ --------------------------------- 2.6/15.4 MB 6.5 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 3.0/15.4 MB 6.6 MB/s eta 0:00:02\n",
            "     -------- ------------------------------- 3.3/15.4 MB 6.3 MB/s eta 0:00:02\n",
            "     --------- ------------------------------ 3.5/15.4 MB 6.3 MB/s eta 0:00:02\n",
            "     --------- ------------------------------ 3.8/15.4 MB 6.3 MB/s eta 0:00:02\n",
            "     ---------- ----------------------------- 4.1/15.4 MB 6.4 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 4.4/15.4 MB 6.3 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 4.7/15.4 MB 6.4 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 5.0/15.4 MB 6.4 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 5.4/15.4 MB 6.5 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 5.7/15.4 MB 6.5 MB/s eta 0:00:02\n",
            "     --------------- ------------------------ 6.0/15.4 MB 6.5 MB/s eta 0:00:02\n",
            "     ---------------- ----------------------- 6.3/15.4 MB 6.5 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 6.7/15.4 MB 6.6 MB/s eta 0:00:02\n",
            "     ------------------ --------------------- 7.1/15.4 MB 6.6 MB/s eta 0:00:02\n",
            "     ------------------- -------------------- 7.4/15.4 MB 6.7 MB/s eta 0:00:02\n",
            "     -------------------- ------------------- 7.8/15.4 MB 6.6 MB/s eta 0:00:02\n",
            "     --------------------- ------------------ 8.1/15.4 MB 6.7 MB/s eta 0:00:02\n",
            "     ---------------------- ----------------- 8.5/15.4 MB 6.8 MB/s eta 0:00:02\n",
            "     ---------------------- ----------------- 8.8/15.4 MB 6.8 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 9.1/15.4 MB 6.8 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 9.5/15.4 MB 6.8 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 9.8/15.4 MB 6.7 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 10.2/15.4 MB 6.8 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 10.6/15.4 MB 6.9 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 10.9/15.4 MB 6.9 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 11.2/15.4 MB 7.0 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 11.6/15.4 MB 7.0 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 12.0/15.4 MB 7.0 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 12.3/15.4 MB 7.1 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 12.6/15.4 MB 7.1 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 13.0/15.4 MB 7.1 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 13.4/15.4 MB 7.2 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 13.7/15.4 MB 7.3 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 14.0/15.4 MB 7.3 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 14.3/15.4 MB 7.3 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 14.6/15.4 MB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 15.0/15.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  15.4/15.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  15.4/15.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 15.4/15.4 MB 7.1 MB/s eta 0:00:00\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4\n",
            "  Downloading llama_index_agent_openai-0.2.5-py3-none-any.whl (13 kB)\n",
            "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5\n",
            "  Downloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "     ------ --------------------------------- 0.3/2.0 MB 10.6 MB/s eta 0:00:01\n",
            "     -------------- ------------------------- 0.7/2.0 MB 8.9 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 1.1/2.0 MB 8.4 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 1.4/2.0 MB 8.1 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 1.8/2.0 MB 7.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.0/2.0 MB 7.4 MB/s eta 0:00:00\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.13\n",
            "  Downloading llama_index_llms_openai-0.1.19-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: Pillow in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from sentence-transformers) (9.5.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from sentence-transformers) (1.10.0)\n",
            "Collecting huggingface-hub>=0.15.1\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "     ---------------------------------------- 0.0/401.2 kB ? eta -:--:--\n",
            "     -------------------------------- ----- 348.2/401.2 kB 7.2 MB/s eta 0:00:01\n",
            "     -------------------------------------- 401.2/401.2 kB 6.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
            "Collecting transformers<5.0.0,>=4.34.0\n",
            "  Using cached transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "     ---------------------------------------- 0.0/49.3 kB ? eta -:--:--\n",
            "     ---------------------------------------- 49.3/49.3 kB ? eta 0:00:00\n",
            "Collecting typing-inspect<1,>=0.4.0\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting fsspec>=2023.5.0\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "     ---------------------------------------- 0.0/316.1 kB ? eta -:--:--\n",
            "     ------------------------------ ------- 256.0/316.1 kB 5.3 MB/s eta 0:00:01\n",
            "     -------------------------------------- 316.1/316.1 kB 6.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.1)\n",
            "Collecting packaging>=20.9\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "     ---------------------------------------- 0.0/53.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 53.0/53.0 kB ? eta 0:00:00\n",
            "Collecting jsonpatch<2.0,>=1.33\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14\n",
            "  Downloading orjson-3.10.3-cp310-none-win_amd64.whl (138 kB)\n",
            "     ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
            "     -------------------------------------- 138.8/138.8 kB 4.1 MB/s eta 0:00:00\n",
            "Collecting openai>=1.14.0\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "     ---------------------------------------- 0.0/320.6 kB ? eta -:--:--\n",
            "     -------------------------- ----------- 225.3/320.6 kB 4.6 MB/s eta 0:00:01\n",
            "     -------------------------------------- 320.6/320.6 kB 4.9 MB/s eta 0:00:00\n",
            "Collecting networkx>=3.0\n",
            "  Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
            "     ----- ---------------------------------- 0.2/1.7 MB 6.9 MB/s eta 0:00:01\n",
            "     -------- ------------------------------- 0.4/1.7 MB 3.8 MB/s eta 0:00:01\n",
            "     ------------ --------------------------- 0.5/1.7 MB 3.6 MB/s eta 0:00:01\n",
            "     ---------------- ----------------------- 0.7/1.7 MB 4.0 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 0.9/1.7 MB 4.0 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 1.1/1.7 MB 4.1 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 1.3/1.7 MB 4.0 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 1.5/1.7 MB 4.1 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 1.7/1.7 MB 4.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.7/1.7 MB 3.9 MB/s eta 0:00:00\n",
            "Collecting aiohttp<4.0.0,>=3.8.3\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-win_amd64.whl (370 kB)\n",
            "     ---------------------------------------- 0.0/370.7 kB ? eta -:--:--\n",
            "     ---------------- --------------------- 163.8/370.7 kB 9.6 MB/s eta 0:00:01\n",
            "     -------------------------------------- 370.7/370.7 kB 5.7 MB/s eta 0:00:00\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "     ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
            "     ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
            "Collecting tiktoken>=0.3.3\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-win_amd64.whl (798 kB)\n",
            "     ---------------------------------------- 0.0/798.9 kB ? eta -:--:--\n",
            "     ------ ------------------------------- 143.4/798.9 kB 4.3 MB/s eta 0:00:01\n",
            "     ------------------- ------------------ 419.8/798.9 kB 5.3 MB/s eta 0:00:01\n",
            "     ----------------------------- -------- 614.4/798.9 kB 4.8 MB/s eta 0:00:01\n",
            "     -------------------------------------- 798.9/798.9 kB 4.6 MB/s eta 0:00:00\n",
            "Collecting deprecated>=1.2.9.3\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: pandas in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.5.3)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Collecting PyYAML>=5.3\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
            "     ---------------------------------------- 0.0/145.3 kB ? eta -:--:--\n",
            "     -------------------------------------- 145.3/145.3 kB 8.4 MB/s eta 0:00:00\n",
            "Collecting nest-asyncio<2.0.0,>=1.5.8\n",
            "  Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "     ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 75.6/75.6 kB 4.4 MB/s eta 0:00:00\n",
            "Collecting spacy<4.0.0,>=3.7.1\n",
            "  Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl (12.1 MB)\n",
            "     ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.2/12.1 MB 9.6 MB/s eta 0:00:02\n",
            "     - -------------------------------------- 0.4/12.1 MB 6.8 MB/s eta 0:00:02\n",
            "     -- ------------------------------------- 0.7/12.1 MB 5.9 MB/s eta 0:00:02\n",
            "     --- ------------------------------------ 0.9/12.1 MB 5.8 MB/s eta 0:00:02\n",
            "     --- ------------------------------------ 1.1/12.1 MB 5.2 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 1.4/12.1 MB 5.5 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 1.7/12.1 MB 5.5 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 2.0/12.1 MB 5.6 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 2.3/12.1 MB 5.8 MB/s eta 0:00:02\n",
            "     -------- ------------------------------- 2.6/12.1 MB 6.0 MB/s eta 0:00:02\n",
            "     --------- ------------------------------ 2.9/12.1 MB 5.9 MB/s eta 0:00:02\n",
            "     ---------- ----------------------------- 3.2/12.1 MB 6.1 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 3.5/12.1 MB 5.8 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 3.8/12.1 MB 5.9 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 4.1/12.1 MB 6.0 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 4.4/12.1 MB 6.1 MB/s eta 0:00:02\n",
            "     --------------- ------------------------ 4.7/12.1 MB 6.0 MB/s eta 0:00:02\n",
            "     ---------------- ----------------------- 5.0/12.1 MB 6.1 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 5.3/12.1 MB 6.2 MB/s eta 0:00:02\n",
            "     ------------------ --------------------- 5.6/12.1 MB 6.1 MB/s eta 0:00:02\n",
            "     ------------------- -------------------- 5.9/12.1 MB 6.2 MB/s eta 0:00:02\n",
            "     -------------------- ------------------- 6.2/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 6.5/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 6.8/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 7.1/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 7.4/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 7.7/12.1 MB 6.2 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 8.0/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.2/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.4/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 8.7/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.0/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 9.4/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 9.6/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 9.9/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 10.2/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 10.4/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 10.7/12.1 MB 6.1 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.0/12.1 MB 6.2 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 11.3/12.1 MB 6.2 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 11.6/12.1 MB 6.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  11.8/12.1 MB 6.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.1/12.1 MB 6.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.1/12.1 MB 6.1 MB/s eta 0:00:00\n",
            "Collecting jsonpath-ng<2.0.0,>=1.6.0\n",
            "  Downloading jsonpath_ng-1.6.1-py3-none-any.whl (29 kB)\n",
            "Collecting nltk<4.0.0,>=3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "     ----- ---------------------------------- 0.2/1.5 MB 6.1 MB/s eta 0:00:01\n",
            "     -------------- ------------------------- 0.5/1.5 MB 6.7 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 0.8/1.5 MB 6.6 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 1.1/1.5 MB 6.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------  1.5/1.5 MB 6.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.5/1.5 MB 6.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: wrapt in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.14.1)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.18\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "     ---------------------------------------- 0.0/141.9 kB ? eta -:--:--\n",
            "     -------------------------------------- 141.9/141.9 kB 4.1 MB/s eta 0:00:00\n",
            "Collecting requests<3,>=2\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting pypdf<5.0.0,>=4.0.1\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "     ---------------------------------------- 0.0/290.4 kB ? eta -:--:--\n",
            "     ------------------------------------ - 276.5/290.4 kB 8.3 MB/s eta 0:00:01\n",
            "     -------------------------------------- 290.4/290.4 kB 6.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0\n",
            "  Downloading llama_parse-0.4.3-py3-none-any.whl (7.7 kB)\n",
            "Collecting annotated-types>=0.4.0\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.18.2\n",
            "  Downloading pydantic_core-2.18.2-cp310-none-win_amd64.whl (1.9 MB)\n",
            "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
            "     ------ --------------------------------- 0.3/1.9 MB 6.3 MB/s eta 0:00:01\n",
            "     ------------- -------------------------- 0.6/1.9 MB 6.5 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 1.0/1.9 MB 6.7 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 1.3/1.9 MB 6.8 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 1.6/1.9 MB 7.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------  1.9/1.9 MB 6.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.9/1.9 MB 6.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tqdm->gpt4all) (0.4.6)\n",
            "Collecting safetensors>=0.4.1\n",
            "  Downloading safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
            "     ---------------------------------------- 0.0/287.4 kB ? eta -:--:--\n",
            "     -------------------------------------  286.7/287.4 kB 8.9 MB/s eta 0:00:01\n",
            "     -------------------------------------- 287.4/287.4 kB 6.0 MB/s eta 0:00:00\n",
            "Collecting tokenizers<0.20,>=0.19\n",
            "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
            "     ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "     ---- ----------------------------------- 0.3/2.2 MB 5.7 MB/s eta 0:00:01\n",
            "     --------- ------------------------------ 0.5/2.2 MB 6.7 MB/s eta 0:00:01\n",
            "     -------------- ------------------------- 0.8/2.2 MB 6.6 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 1.1/2.2 MB 6.3 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 1.4/2.2 MB 6.0 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 1.7/2.2 MB 6.4 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 1.9/2.2 MB 6.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  2.2/2.2 MB 6.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.2/2.2 MB 5.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.7.9)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting ply\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "     ---------------------------------------- 0.0/49.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 49.6/49.6 kB 2.5 MB/s eta 0:00:00\n",
            "Collecting httpcore==1.*\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "     ---------------------------------------- 0.0/77.9 kB ? eta -:--:--\n",
            "     ---------------------------------------- 77.9/77.9 kB 4.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: sniffio in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.3.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.7.1)\n",
            "Collecting h11<0.15,>=0.13\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Requirement already satisfied: click in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (8.0.4)\n",
            "Collecting distro<2,>=1.7.0\n",
            "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1\n",
            "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
            "     ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
            "     -------------------------------------- 182.0/182.0 kB 5.5 MB/s eta 0:00:00\n",
            "Collecting thinc<8.3.0,>=8.2.2\n",
            "  Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
            "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "     -------- ------------------------------- 0.3/1.5 MB 6.5 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 0.7/1.5 MB 7.2 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 1.0/1.5 MB 7.4 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 1.3/1.5 MB 7.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.5/1.5 MB 6.8 MB/s eta 0:00:00\n",
            "Collecting typer<0.10.0,>=0.3.0\n",
            "  Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
            "     ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 46.0/46.0 kB 2.2 MB/s eta 0:00:00\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
            "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
            "     ---------------------------------------- 0.0/481.9 kB ? eta -:--:--\n",
            "     -------------------- ----------------- 266.2/481.9 kB 8.3 MB/s eta 0:00:01\n",
            "     -------------------------------------- 481.9/481.9 kB 6.0 MB/s eta 0:00:00\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
            "     ---------------------------------------- 0.0/122.2 kB ? eta -:--:--\n",
            "     -------------------------------------- 122.2/122.2 kB 7.5 MB/s eta 0:00:00\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Collecting weasel<0.4.0,>=0.1.0\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "     ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
            "     ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
            "Requirement already satisfied: setuptools in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (65.6.3)\n",
            "Collecting smart-open<7.0.0,>=5.2.1\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "     ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 57.0/57.0 kB 2.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.8.2)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.2.0)\n",
            "Collecting language-data>=1.2\n",
            "  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
            "     ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
            "     -- ------------------------------------- 0.3/5.4 MB 7.0 MB/s eta 0:00:01\n",
            "     ---- ----------------------------------- 0.6/5.4 MB 6.8 MB/s eta 0:00:01\n",
            "     ------- -------------------------------- 1.0/5.4 MB 6.9 MB/s eta 0:00:01\n",
            "     --------- ------------------------------ 1.3/5.4 MB 6.8 MB/s eta 0:00:01\n",
            "     ------------ --------------------------- 1.6/5.4 MB 7.0 MB/s eta 0:00:01\n",
            "     -------------- ------------------------- 2.0/5.4 MB 6.9 MB/s eta 0:00:01\n",
            "     ----------------- ---------------------- 2.3/5.4 MB 7.0 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 2.7/5.4 MB 7.1 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 3.0/5.4 MB 7.1 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 3.3/5.4 MB 7.1 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 3.7/5.4 MB 7.1 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 4.1/5.4 MB 7.2 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 4.4/5.4 MB 7.2 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 4.7/5.4 MB 7.2 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 5.1/5.4 MB 7.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  5.4/5.4 MB 7.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 5.4/5.4 MB 6.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.16.0)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
            "     ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.2/6.6 MB 5.6 MB/s eta 0:00:02\n",
            "     --- ------------------------------------ 0.6/6.6 MB 7.4 MB/s eta 0:00:01\n",
            "     ----- ---------------------------------- 0.9/6.6 MB 7.2 MB/s eta 0:00:01\n",
            "     ------- -------------------------------- 1.3/6.6 MB 7.4 MB/s eta 0:00:01\n",
            "     --------- ------------------------------ 1.6/6.6 MB 7.2 MB/s eta 0:00:01\n",
            "     ----------- ---------------------------- 1.9/6.6 MB 7.1 MB/s eta 0:00:01\n",
            "     ------------- -------------------------- 2.2/6.6 MB 7.0 MB/s eta 0:00:01\n",
            "     --------------- ------------------------ 2.5/6.6 MB 7.0 MB/s eta 0:00:01\n",
            "     ----------------- ---------------------- 2.9/6.6 MB 7.0 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 3.2/6.6 MB 7.1 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 3.6/6.6 MB 7.2 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 3.9/6.6 MB 7.1 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 4.3/6.6 MB 7.2 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 4.6/6.6 MB 7.2 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 4.9/6.6 MB 7.2 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 5.3/6.6 MB 7.2 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 5.6/6.6 MB 7.1 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 5.9/6.6 MB 7.1 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 6.3/6.6 MB 7.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------  6.6/6.6 MB 7.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 6.6/6.6 MB 6.9 MB/s eta 0:00:00\n",
            "Collecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "     ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 45.0/45.0 kB 2.2 MB/s eta 0:00:00\n",
            "Collecting marisa-trie>=0.7.7\n",
            "  Downloading marisa_trie-1.1.1-cp310-cp310-win_amd64.whl (152 kB)\n",
            "     ---------------------------------------- 0.0/152.7 kB ? eta -:--:--\n",
            "     -------------------------------------- 152.7/152.7 kB 8.9 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.75-cp310-cp310-win_amd64.whl size=3321560 sha256=f82350d4137a82c241717fd6a64c962311c4b0b9fe3f8006620ec701eb9709e0\n",
            "  Stored in directory: c:\\users\\gutsc\\appdata\\local\\pip\\cache\\wheels\\5e\\df\\9a\\e4bb2e48bfa64fb174f0f786296c8507dbebea2a112f1adf8d\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: striprtf, ply, dirtyjson, cymem, wasabi, typing-inspect, tqdm, spacy-loggers, spacy-legacy, smart-open, safetensors, requests, PyYAML, pypdf, pydantic-core, packaging, orjson, networkx, nest-asyncio, murmurhash, marisa-trie, jsonpointer, jsonpath-ng, h11, fsspec, distro, deprecated, cloudpathlib, catalogue, blis, annotated-types, typer, tiktoken, srsly, pydantic, preshed, nltk, marshmallow, llama-cpp-python, language-data, jsonpatch, huggingface-hub, httpcore, gpt4all, aiohttp, tokenizers, langsmith, langcodes, httpx, dataclasses-json, confection, weasel, transformers, thinc, openai, llamaindex-py-client, langchain-core, spacy, sentence-transformers, llama-index-legacy, langchain-text-splitters, langchain-community, llama-index-core, langchain, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.28.1\n",
            "    Uninstalling requests-2.28.1:\n",
            "      Successfully uninstalled requests-2.28.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 22.0\n",
            "    Uninstalling packaging-22.0:\n",
            "      Successfully uninstalled packaging-22.0\n",
            "  Attempting uninstall: nest-asyncio\n",
            "    Found existing installation: nest-asyncio 1.5.6\n",
            "    Uninstalling nest-asyncio-1.5.6:\n",
            "      Successfully uninstalled nest-asyncio-1.5.6\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.4.0\n",
            "    Uninstalling fsspec-2023.4.0:\n",
            "      Successfully uninstalled fsspec-2023.4.0\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.1.78\n",
            "    Uninstalling llama_cpp_python-0.1.78:\n",
            "      Successfully uninstalled llama_cpp_python-0.1.78\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] Zugriff verweigert: 'C:\\\\Users\\\\gutsc\\\\anaconda3\\\\envs\\\\tf_GPU\\\\Lib\\\\site-packages\\\\~lama_cpp\\\\llama.dll'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade llama-cpp-python langchain gpt4all llama-index sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: huggingface-cli <command> [<args>]\n",
            "huggingface-cli: error: argument {login,whoami,logout,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache}: invalid choice: 'download' (choose from 'login', 'whoami', 'logout', 'repo', 'lfs-enable-largefiles', 'lfs-multipart-upload', 'scan-cache', 'delete-cache')\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli download TheBloke/Llama-2–7b-Chat-GGUF llama-2–7b-chat.Q5_K_S.gguf — local-dir . — local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python==0.1.48\n",
            "  Downloading llama_cpp_python-0.1.48.tar.gz (1.1 MB)\n",
            "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
            "     ------- -------------------------------- 0.2/1.1 MB 6.3 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 0.5/1.1 MB 6.7 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 0.8/1.1 MB 6.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------  1.1/1.1 MB 6.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.1/1.1 MB 6.0 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting resolved\n",
            "  Downloading reSolved-0.0.3-py3-none-any.whl (3.1 kB)\n",
            "Collecting my\n",
            "  Downloading my-1.3.0.tar.gz (529 bytes)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting issue\n",
            "  Downloading issue-0.1.0-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from llama-cpp-python==0.1.48) (4.11.0)\n",
            "Building wheels for collected packages: llama-cpp-python, my\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.48-cp310-cp310-win_amd64.whl size=234012 sha256=92555187698e96237620b01015e681381a9aaf28eb055e8617992fbb475e938e\n",
            "  Stored in directory: c:\\users\\gutsc\\appdata\\local\\pip\\cache\\wheels\\eb\\06\\0e\\6ae7b299ed252075128644d31384cac683e9fd768a8538c6be\n",
            "  Building wheel for my (setup.py): started\n",
            "  Building wheel for my (setup.py): finished with status 'done'\n",
            "  Created wheel for my: filename=my-1.3.0-py3-none-any.whl size=1155 sha256=d9851b47f8471a4a1a36a32828b98296f4509a2834d0d0aa9acb5c92bcb1c7eb\n",
            "  Stored in directory: c:\\users\\gutsc\\appdata\\local\\pip\\cache\\wheels\\eb\\f0\\38\\8bb5f0b6893bd56ebe1c3c937afd405fbba07fbb9dd94c9a7f\n",
            "Successfully built llama-cpp-python my\n",
            "Installing collected packages: my, resolved, llama-cpp-python, issue\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.75\n",
            "    Uninstalling llama_cpp_python-0.2.75:\n",
            "      Successfully uninstalled llama_cpp_python-0.2.75\n",
            "Successfully installed issue-0.1.0 llama-cpp-python-0.1.48 my-1.3.0 resolved-0.0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python==0.1.48\n",
            "  Using cached llama_cpp_python-0.1.48.tar.gz (1.1 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from llama-cpp-python==0.1.48) (4.11.0)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.48-cp310-cp310-win_amd64.whl size=234014 sha256=1c5f96193908d30b93a9d88c6f185d41d0b3f9d85ca63e884cf453323035c468\n",
            "  Stored in directory: c:\\users\\gutsc\\appdata\\local\\pip\\cache\\wheels\\eb\\06\\0e\\6ae7b299ed252075128644d31384cac683e9fd768a8538c6be\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama-cpp-python 0.2.75\n",
            "    Can't uninstall 'llama-cpp-python'. No files were found to uninstall.\n",
            "Successfully installed llama-cpp-python-0.1.48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Error parsing requirements for llama-cpp-python: [Errno 2] No such file or directory: 'c:\\\\users\\\\gutsc\\\\anaconda3\\\\envs\\\\tf_gpu\\\\lib\\\\site-packages\\\\llama_cpp_python-0.2.75.dist-info\\\\METADATA'\n",
            "    WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "    WARNING: No metadata found in c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\gutsc\\anaconda3\\envs\\tf_gpu\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python==0.1.48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Model path does not exist: /llama-2-7b-chat.Q5_K_S.gguf",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m llama \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/llama-2-7b-chat.Q5_K_S.gguf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\tf_GPU\\lib\\site-packages\\llama_cpp\\llama.py:317\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\u001b[0m\n\u001b[0;32m    311\u001b[0m self.context_params.yarn_beta_slow = (\n\u001b[0;32m    312\u001b[0m     yarn_beta_slow if yarn_beta_slow != 0.0 else 0\n\u001b[0;32m    313\u001b[0m )\n\u001b[0;32m    314\u001b[0m self.context_params.yarn_orig_ctx = yarn_orig_ctx if yarn_orig_ctx != 0 else 0\n\u001b[0;32m    315\u001b[0m self.context_params.logits_all = (\n\u001b[0;32m    316\u001b[0m     logits_all if draft_model is None else True\n\u001b[1;32m--> 317\u001b[0m )  # Must be set to True for speculative decoding\n\u001b[0;32m    318\u001b[0m self.context_params.embeddings = embedding # TODO: Rename to embeddings\n\u001b[0;32m    319\u001b[0m self.context_params.offload_kqv = offload_kqv\n",
            "\u001b[1;31mValueError\u001b[0m: Model path does not exist: /llama-2-7b-chat.Q5_K_S.gguf"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model\n",
        "llama = Llama(model_path='/llama-2-7b-chat.Q5_K_S.gguf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n\u001b[0;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/Phi-3-mini-4k-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Phi-3-mini-4k-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you provide ways to eat combinations of bananas and dragonfruits?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     16\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     17\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat about solving an 2x + 3 = 7 equation?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     18\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\tf_GPU\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:455\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m    454\u001b[0m     module_file, class_name \u001b[38;5;241m=\u001b[39m class_ref\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 455\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[0;32m    456\u001b[0m         pretrained_model_name_or_path, module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    459\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    460\u001b[0m     )\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n",
            "File \u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\tf_GPU\\lib\\site-packages\\transformers\\dynamic_module_utils.py:363\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[1;34m(pretrained_model_name_or_path, module_file, class_name, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03mExtracts a class from a module file, present in the local folder or repository of a model.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03mcls = get_class_from_dynamic_module(\"sgugger/my-bert-model\", \"modeling.py\", \"MyBertModel\")\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m final_module \u001b[38;5;241m=\u001b[39m \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\tf_GPU\\lib\\site-packages\\transformers\\dynamic_module_utils.py:237\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[1;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m modules_needed \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_module_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[0;32m    240\u001b[0m full_submodule \u001b[38;5;241m=\u001b[39m TRANSFORMERS_DYNAMIC_MODULE_NAME \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m submodule\n",
            "File \u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\tf_GPU\\lib\\site-packages\\transformers\\dynamic_module_utils.py:134\u001b[0m, in \u001b[0;36mcheck_imports\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    131\u001b[0m         missing_packages\u001b[38;5;241m.\u001b[39mappend(imp)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_packages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis modeling file requires the following packages that were not found in your environment: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_relative_imports(filename)\n",
            "\u001b[1;31mImportError\u001b[0m: This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
        "    device_map=\"cuda\", \n",
        "    torch_dtype=\"auto\", \n",
        "    trust_remote_code=True, \n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
        "]\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"A small step\"\n",
        "response = llama(prompt)\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XwqtiOveOiM"
      },
      "source": [
        "## Rekurrente Neuronale Netze in Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDLF1V06fK4m"
      },
      "source": [
        "Im November 2016 revolutionierte Google die maschinelle Übersetzung mit der Einführung des GNMT-Systems (Google Neural Machine Translation). Dieses System nutzte eine innovative Encoder-Decoder-Architektur mit RNNs (LSTMs) und einer Aufmerksamkeitsmechanik. Durch den Einsatz von neuronalen Netzwerken verbesserte es die Übersetzungsgenauigkeit deutlich im Vergleich zum vorherigen statistischen Ansatz. Jedoch setzte Google 2017 ein noch effizienteres Modell ein - den Transformer. Dieser innovative Ansatz basierte ausschließlich auf Aufmerksamkeitsmechanismen, wie im hochgelobten Paper \"Attention is All You Need\" beschrieben. Vor der Einführung von GNMT verließ sich Google Translate auf herkömmliche statistische Methoden zur maschinellen Übersetzung. Diese bahnbrechenden Fortschritte haben die Präzision und Leistungsfähigkeit von Google Translate erheblich gesteigert."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT990VVWfTiy"
      },
      "source": [
        "Auch wenn RNNs nicht mehr State of the Art für NLP ist, lässt sich hier doch einiges veranschaulichen. Daher versuche ich im folgenden den IMDB Datensatz mit Hilfe von RNNs zu analysieren und Vorhersagen zu machen, ob eine Kritik z.B. positiv oder negativ ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-8dnGg4ftUU"
      },
      "source": [
        "### IMDB Datensatz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jirgQw09gUW1"
      },
      "source": [
        "Der IMDB-Datensatz ist eine wertvolle Ressource für die natürliche Sprachverarbeitung (NLP). Er umfasst eine große Anzahl von Filmrezensionen, die jeweils mit Bewertungen versehen sind. Dieser Datensatz wird häufig in der Sentimentanalyse eingesetzt, um den sentimentalen Gehalt eines Textes zu klassifizieren, ob er positiv oder negativ ist. Mit Hilfe des IMDB-Datensatzes können Forscher und Entwickler NLP-Algorithmen trainieren und verbessern, um Texte besser zu verstehen und Bewertungen automatisch zu analysieren.\n",
        "\n",
        "Es ist sozusagen das **Hallo Welt!** des NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKSD7phogfih"
      },
      "source": [
        "Im folgenden werde ich versuchen, dass Beispiel IMDB in Python umzusetzen und vorherzusagen, ob eine Kritik positiv oder negativ ist. Ich fange mit den wichtigsen Imports an."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0mNuVTOZFbk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Laden des IMDB-Datensatzes\n",
        "vocab_size = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Vorbereiten der Daten\n",
        "max_len = 200\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Erstellen des Modells\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 32, input_length=max_len),\n",
        "    keras.layers.LSTM(64),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training des Modells\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)\n",
        "\n",
        "# Vorhersagen treffen\n",
        "text = \"This movie was amazing!\"\n",
        "sequence = imdb.get_word_index()\n",
        "input_text = [sequence[word.lower()] if word.lower() in sequence else 0 for word in text.split()]\n",
        "input_text = pad_sequences([input_text], maxlen=max_len)\n",
        "prediction = model.predict(input_text)[0][0]\n",
        "\n",
        "# Ausgabe der Vorhersage\n",
        "if prediction >= 0.5:\n",
        "    print(\"Positive Sentiment\")\n",
        "else:\n",
        "    print(\"Negative Sentiment\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1XwqtiOveOiM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
