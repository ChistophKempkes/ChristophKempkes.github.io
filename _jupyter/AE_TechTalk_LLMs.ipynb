{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1XwqtiOveOiM"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models"
      ],
      "metadata": {
        "id": "znsh8PV8avg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dass es sich bei Large Language Models (LLMs) nicht um Magie oder eine starke künstliche Intelligenz (oder gar einer Artificial Generel Intelligence bzw. AGI) sondern um reine Stastistik handelt, sollte mittlerweile allen bekannt sein.\n",
        "\n",
        "Ein LLM macht doch nichts anderes, als an den vorhandenen Text ein weiteres Wort anzuhängen. Wort für Wort. Basierend auf Statistiken, die angeben, welches Wort die höchste Wahrscheinlichkeit hat, auf das oder die vorangegangenen Worte zu folgen. **Oder?!**\n",
        "\n",
        "Ich möchte versuchen mich der Funktionsweise von Natural Language Processing (NLP) und LLMs anzunähern. Dabei werde ich über Begriffe, wie **Embeddings**, **Tokens**, **Attention** oder **Transformer** stolpern und versuchen zu erklären was hier passiert. Ich werde auch kurz auf **Rekurrente Neuronale Netze** eingehen, spielten sie doch zum Beispiel im Google Translator in Form des **Google Neural Machine Translation (GNMT)** Systems eine Rolle."
      ],
      "metadata": {
        "id": "hBiSMmcdbweO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ein Wort nach dem anderem?"
      ],
      "metadata": {
        "id": "EoiQYG5t_Yhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language Models versuchen an den gegebenen Text sinnvoll Wörter anzuhängen. Unter sinnvoll versteht man, was man von einer Person zu schreiben erwarten würde, wenn diese Person Milliarden von Text, Büchern, Internetseiten usw. gelesen und sich dann gemerkt hätte, welches Wort am häufigsten auf den vorangegangenen Text oder das vorangegangene Wort folgte."
      ],
      "metadata": {
        "id": "V2CsuNRv_b3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4"
      ],
      "metadata": {
        "id": "Mf7VT7PpJ6TP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
      ],
      "metadata": {
        "id": "sPEHTc0xNAzR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import llama_cpp"
      ],
      "metadata": {
        "id": "LnGs2BkUNIf9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "DJaKePJRNKfc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "llm = llama_cpp.Llama(model_path)"
      ],
      "metadata": {
        "id": "8Zly0bT8POrs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate a response and print probabilities\n",
        "def generate_response(prompt):\n",
        "    output = llm(prompt)\n",
        "    print(\"Response:\", output[\"choices\"][0][\"text\"])\n",
        "    print(\"Probabilities:\", output[\"choices\"][0][\"probabilities\"])"
      ],
      "metadata": {
        "id": "sMZGxcIfPgfV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A small step\""
      ],
      "metadata": {
        "id": "NYp5lwtJOoV0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a response and print probabilities\n",
        "generate_response(prompt)"
      ],
      "metadata": {
        "id": "GYJdwa5kPhvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rekurrente Neuronale Netze in Natural Language Processing"
      ],
      "metadata": {
        "id": "1XwqtiOveOiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im November 2016 revolutionierte Google die maschinelle Übersetzung mit der Einführung des GNMT-Systems (Google Neural Machine Translation). Dieses System nutzte eine innovative Encoder-Decoder-Architektur mit RNNs (LSTMs) und einer Aufmerksamkeitsmechanik. Durch den Einsatz von neuronalen Netzwerken verbesserte es die Übersetzungsgenauigkeit deutlich im Vergleich zum vorherigen statistischen Ansatz. Jedoch setzte Google 2017 ein noch effizienteres Modell ein - den Transformer. Dieser innovative Ansatz basierte ausschließlich auf Aufmerksamkeitsmechanismen, wie im hochgelobten Paper \"Attention is All You Need\" beschrieben. Vor der Einführung von GNMT verließ sich Google Translate auf herkömmliche statistische Methoden zur maschinellen Übersetzung. Diese bahnbrechenden Fortschritte haben die Präzision und Leistungsfähigkeit von Google Translate erheblich gesteigert."
      ],
      "metadata": {
        "id": "kDLF1V06fK4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auch wenn RNNs nicht mehr State of the Art für NLP ist, lässt sich hier doch einiges veranschaulichen. Daher versuche ich im folgenden den IMDB Datensatz mit Hilfe von RNNs zu analysieren und Vorhersagen zu machen, ob eine Kritik z.B. positiv oder negativ ist."
      ],
      "metadata": {
        "id": "uT990VVWfTiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMDB Datensatz\n"
      ],
      "metadata": {
        "id": "i-8dnGg4ftUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der IMDB-Datensatz ist eine wertvolle Ressource für die natürliche Sprachverarbeitung (NLP). Er umfasst eine große Anzahl von Filmrezensionen, die jeweils mit Bewertungen versehen sind. Dieser Datensatz wird häufig in der Sentimentanalyse eingesetzt, um den sentimentalen Gehalt eines Textes zu klassifizieren, ob er positiv oder negativ ist. Mit Hilfe des IMDB-Datensatzes können Forscher und Entwickler NLP-Algorithmen trainieren und verbessern, um Texte besser zu verstehen und Bewertungen automatisch zu analysieren.\n",
        "\n",
        "Es ist sozusagen das **Hallo Welt!** des NLP."
      ],
      "metadata": {
        "id": "jirgQw09gUW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im folgenden werde ich versuchen, dass Beispiel IMDB in Python umzusetzen und vorherzusagen, ob eine Kritik positiv oder negativ ist. Ich fange mit den wichtigsen Imports an."
      ],
      "metadata": {
        "id": "nKSD7phogfih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0mNuVTOZFbk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Laden des IMDB-Datensatzes\n",
        "vocab_size = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Vorbereiten der Daten\n",
        "max_len = 200\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Erstellen des Modells\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 32, input_length=max_len),\n",
        "    keras.layers.LSTM(64),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training des Modells\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)\n",
        "\n",
        "# Vorhersagen treffen\n",
        "text = \"This movie was amazing!\"\n",
        "sequence = imdb.get_word_index()\n",
        "input_text = [sequence[word.lower()] if word.lower() in sequence else 0 for word in text.split()]\n",
        "input_text = pad_sequences([input_text], maxlen=max_len)\n",
        "prediction = model.predict(input_text)[0][0]\n",
        "\n",
        "# Ausgabe der Vorhersage\n",
        "if prediction >= 0.5:\n",
        "    print(\"Positive Sentiment\")\n",
        "else:\n",
        "    print(\"Negative Sentiment\")"
      ]
    }
  ]
}