---
title: Mein erster Post
date: 2024-03-24 13:13:00 +/-TTTT
categories: [Maschinelles Lernen]
tags: [ML, DL, AI]     # TAG names should always be lowercase
toc: false
---

# Theorie und Formeln hinter dem Entscheidungsbaum

In den letzten beiden Artikeln haben wir gelernt, wie Entscheidungsbäume und Random Forests programmiert werden können. In diesem Artikel soll es um die Theorie dahinter gehen. Doch zunächst eine kleine Wiederholung.

Entscheidungsbäume sind ein bekanntes Modell im maschinellen Lernen und werden oft für Klassifikations- und Regressionsprobleme verwendet. Sie stellen eine Hierarchie von Entscheidungen und Vorhersagen dar, die auf bestimmten Merkmalen und Regeln basieren.

Bei einem Random Forest wiederum handelt es sich um mehrere unkorrelierte Entscheidungsbäume, die während des Lernprozesses unter einer bestimmten Randomisierung entstanden sind und mit zufälligen Untermengen der Ausgangsdaten trainiert wurden. Die endgültige Vorhersage des Modells wird durch die Abstimmung der Vorhersagen aller Entscheidungsbäume ermittelt.

Doch nun zu den Konzepten und der Theorie hinter den Entscheidungsbäumen und somit letztlich auch den Random Forests.

## Informationsgewinn und Entropie

Eines der wichtigsten Konzepte bei Entscheidungsbäumen ist die Informationsgewinnung (Information Gain). Dies ist eine Messgröße, die bestimmt, welches Merkmal am besten geeignet ist, um die Daten zu teilen. Der Information Gain wird berechnet, indem man den Unterschied zwischen dem ursprünglichen Entropiewert und dem Entropiewert nach der Teilung berechnet.

### Entropie

Der Entropiewert misst die Unordnung oder Heterogenität einer Gruppe von Beobachtungen. Eine höhere Entropie bedeutet eine höhere Unordnung und eine geringere Vorhersagegenauigkeit. Der Entropiewert wird berechnet, indem man die Wahrscheinlichkeit jeder Klasse in der Gruppe und diese Werte logarithmisch berechnet und summiert.

Die Entropie einer diskreten Zufallsvariable X mit möglichen Ausprägungen {x_1, x_2, …, x_n} und Wahrscheinlichkeiten {p_1, p_2, …, p_n} ist definiert als:

$H(x) = $

### Information Gain

Der Information Gain wird berechnet, indem man den Unterschied zwischen dem ursprünglichen Entropiewert und dem Entropiewert nach der Teilung berechnet. Zum Beispiel teilen wir eine Gruppe von Beobachtungen basierend auf einem bestimmten Merkmal, wie etwa dem Alter. Wenn das Alter ein gutes Teilungsmerkmal ist, werden die Beobachtungen in klarere und homogenere Gruppen unterteilt, was zu einer geringeren Entropie führt. Der Information Gain ist dann der Unterschied zwischen dem ursprünglichen Entropiewert und dem Entropiewert nach der Teilung.

Der Information Gain, der von einer Entscheidung A abhängt und den Unterschied in der Entropie vor und nach der Entscheidung widerspiegelt, ist definiert als:

$IG(A) = $

wobei S die Menge aller Beispiele ist, die entschieden werden sollen, t die Menge der Beispiele, bei denen die Entscheidung A getroffen wurde, p(t) die Wahrscheinlichkeit, dass ein Beispiel in t ist, und H(t) die Entropie von t ist.

Mit anderen Worten misst der Information Gain, wie viel Entropie verringert wird, wenn eine bestimmte Entscheidung getroffen wird, im Vergleich zur Entropie der ursprünglichen Menge von Beispielen. Ein hoher Information Gain bedeutet, dass eine Entscheidung nützlicher ist, da sie mehr Entropie verringert und damit mehr Information bereitstellt.